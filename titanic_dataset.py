# -*- coding: utf-8 -*-
"""Titanic_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lE2eD-9i6kgSJVFj5jiEDNLcpUE7zv9K
"""

import numpy as np
import pandas as pd

from sklearn.tree import DecisionTreeClassifier, export_graphviz
import graphviz
from subprocess import check_call

import os
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn import tree
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

from IPython.display import Image

df = sns.load_dataset("titanic")
df.head()

df.isnull().sum()

df['age'].fillna(df['age'].mean(), inplace=True)
df.drop(columns='deck', axis=1, inplace=True) # deck 컬럼의 결측치 비율 = 688/891 = 0.7722 -> 삭제
df.drop(columns='embark_town', axis=1, inplace=True) # embarked와 겹치는 embark_town 열 삭제
df.drop(columns='alive', axis=1, inplace=True) # survived와 겹치는 alive 열 삭제

most_frequent = df['embarked'].value_counts(dropna=True).idxmax()
df['embarked'].fillna(most_frequent, inplace=True)

df.isnull().sum()

df.info()

new_df = pd.get_dummies(df, dtype=int)
new_df.head()

correlation_matrix = new_df.corr()
survived_correlation = correlation_matrix['survived'].sort_values(ascending=False)
print(abs(survived_correlation).sort_values(ascending=False))

df.columns

new_df = df[['survived', 'pclass', 'sex', 'fare', 'embarked', 'class', 'alone']]
new_df = pd.get_dummies(new_df, dtype=int)
new_df.head()

new_df.describe()

"""# Decision Tree 모델 적용"""

# titanic 데이터셋 로드
x = new_df.drop('survived', axis=1)
y = new_df['survived']

print(x.shape)
print(y.shape)

# gini로 결과가 나옴
tree_model_gini = DecisionTreeClassifier()
tree_model_gini.fit(x, y)

dot_data_decision = export_graphviz(
    tree_model_gini,
    out_file = './titanic_tree_model.dot',
    feature_names=x.columns.tolist(),
    class_names=y.map({1: 'Survived', 0: 'Dead'}).astype(str).unique(),
    rounded=True, # 사각형 끝을 둥글게
    filled=True # 사각형 안 색깔 채우기
)

# 예측한 모형 png로 바꿔서, 시각화 하기
check_call(['dot','-Tpng','titanic_tree_model.dot','-o','OutputFile_all_gini.png'])

# entropy로 결과가 나옴
tree_model_entropy = DecisionTreeClassifier(criterion="entropy")
tree_model_entropy.fit(x, y)

dot_data_decision = export_graphviz(
    tree_model_entropy,
    out_file = './titanic_tree_model.dot',
    feature_names=x.columns.tolist(),
    class_names=y.map({1: 'Survived', 0: 'Dead'}).astype(str).unique(),
    rounded=True,
    filled=True
)

# 예측한 모형 png로 바꿔서, 시각화 하기
check_call(['dot','-Tpng','titanic_tree_model.dot','-o','OutputFile_all_entropy.png'])

# [pruning] decision tree 모형 인스턴스 생성 및 하이퍼파라미터(depth) = 4
tree_model_max = DecisionTreeClassifier(criterion="entropy", max_depth=4)  # entropy로 결과가 나옴
tree_model_max.fit(x, y)

dot_data_decision = export_graphviz(
    tree_model_max,
    out_file = './titanic_tree_model.dot',
    feature_names=x.columns.tolist(),
    class_names=y.map({1: 'Survived', 0: 'Dead'}).astype(str).unique(),
    rounded=True,
    filled=True
)

# 예측한 모형 png로 바꿔서, 시각화 하기
check_call(['dot','-Tpng','titanic_tree_model.dot','-o','OutputFile_all_max_depth.png'])

# 1번 의사결정나무 - 지니계수 활용
confusion_matrix(y, tree_model_gini.predict(x))

# 2번 의사결정나무 - entropy 활용
confusion_matrix(y, tree_model_entropy.predict(x))

# 3번 의사결정나무 - 가지치기 작업
confusion_matrix(y, tree_model_max.predict(x))

"""# Test/Train spilit"""

x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, random_state=1)
print(x_train.shape, x_test.shape)

# train dataset
tree_model_train = DecisionTreeClassifier(criterion="entropy")
tree_model_train.fit(x_train,y_train)

# test dataset predict
confusion_matrix(y_test, tree_model_train.predict(x_test))

# 오분류
print(37/223)

# normal tree model
tree_model = DecisionTreeClassifier()
tree_model.fit(x_train, y_train)
y_train_pred = tree_model.predict(x_train)
y_test_pred = tree_model.predict(x_test)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Test score {accuracy_score(y_test_pred,y_test)}')

"""### 1. Pre prunning"""

params = {'max_depth': [2,4,6,8,10],
         'min_samples_split': [2,3],
         'min_samples_leaf': [2,3]}

tree_model = DecisionTreeClassifier()
gscv = GridSearchCV(estimator=tree_model,param_grid=params)
gscv.fit(x_train,y_train)

model = gscv.best_estimator_
model.fit(x_train, y_train)

y_train_pred = model.predict(x_train)
y_test_pred = model.predict(x_test)

print(f'Train score {accuracy_score(y_train_pred, y_train)}')
print(f'Test score {accuracy_score(y_test_pred, y_test)}')

dot_data_decision = export_graphviz(
    model,
    out_file='./titanic_tree_model.dot',
    feature_names=x.columns.tolist(),
    class_names=y.map({1: 'Survived', 0: 'Dead'}).astype(str).unique(),
    rounded=True,
    filled=True
)

check_call(['dot', '-Tpng', 'titanic_tree_model.dot', '-o', 'OutputFile_all_pre_pruning.png'])

Image(filename='OutputFile_all_pre_pruning.png')

"""### 2. Post pruning"""

post_pruning = tree_model.cost_complexity_pruning_path(x_train, y_train)
alphas, impurities = post_pruning.ccp_alphas, post_pruning.impurities

dtcs = []
for ccp_alpha in alphas:
    dtc = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
    dtc.fit(x_train, y_train)
    dtcs.append(dtc)

dtcs = dtcs[:-1]
alphas = alphas[:-1]
train_acc = []
test_acc = []
for d in dtcs:
    y_train_pred = d.predict(x_train)
    y_test_pred = d.predict(x_test)
    train_acc.append(accuracy_score(y_train_pred,y_train))
    test_acc.append(accuracy_score(y_test_pred,y_test))

plt.scatter(alphas,train_acc)
plt.scatter(alphas,test_acc)
plt.plot(alphas,train_acc,label='train_accuracy',drawstyle="steps-post")
plt.plot(alphas,test_acc,label='test_accuracy',drawstyle="steps-post")
plt.legend()
plt.title('Accuracy vs alpha')
plt.show()

dtc_ = DecisionTreeClassifier(ccp_alpha=0.010)
dtc_.fit(x_train,y_train)
y_train_pred = dtc_.predict(x_train)
y_test_pred = dtc_.predict(x_test)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Test score {accuracy_score(y_test_pred,y_test)}')

dot_data_decision = export_graphviz(
    dtc_,
    out_file='./titanic_tree_model.dot',
    feature_names=x.columns.tolist(),
    class_names=y.map({1: 'Survived', 0: 'Dead'}).astype(str).unique(),
    rounded=True,
    filled=True
)

check_call(['dot', '-Tpng', 'titanic_tree_model.dot', '-o', 'OutputFile_all_post_pruning.png'])

Image(filename='OutputFile_all_post_pruning.png')

# pruning 후 decision tree의 크기가 크게 줄어들었으며, post pruning이 pre pruning보다 효율적